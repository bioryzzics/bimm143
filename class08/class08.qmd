---
title: "Class 8: PCA Mini Project"
author: "Noel Lim (PID:A17652474)"
format: gfm
---

It is important to consider scaling your data before analysis such as PCA

```{r}
head(mtcars)
```

```{r}
colMeans(mtcars)
```

```{r}
apply(mtcars,2,sd)
```


```{r}
x <- scale (mtcars)
head(x)
```

```{r}
round(colMeans(x),2)
```

Key-point: It is usually always a good idea to scale your data prior to PCA...

## Breast Cancer Biopsy Analysis

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv("WisconsinCancer.csv",
                    row.names=1)

head(wisc.df)
```

```{r}
diagnosis <- wisc.df[,1]
table(diagnosis)
```

Remove this first `diagnosis` column from the dataset as I don't want to pass this to PCA etc. It is essentially the expert "answer" that we will compare our analysis results to.

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

569 observations

> Q2. How many of the observations have a malignant diagnosis?

212 observations

> Q3. How many variables/features in the data are suffixed with _mean?

10 variables


```{r q3}

length(grep("_mean",colnames(wisc.data)))
```

## Principal  Component Analysis

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp( wisc.data, scale=T )
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.3%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```
This plot is a hot mess, need to generate our own plots for better understanding.

See what is in our PCA result object:

```{r}
attributes(wisc.pr)
```

```{r}
wisc.pr$x
#plot(wisc.pr$x)
```

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=as.factor(diagnosis), xlab="PC1",ylab="PC2")
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = as.factor(diagnosis), xlab = "PC1", ylab = "PC3")
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col = diagnosis) + 
  geom_point()

```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)

```

```{r}
# Variance explained by each principal component: pve
pr.var <- wisc.pr$sdev^2
head(pr.var)
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```
Concave points mean: -0.26085376

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PCs required to explain 80% of the variance of the data

## Hierarchical Clustering
```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

Height = 19
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

## Selecting Number of Clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h = 19)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters10 <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters10, diagnosis)
```

##Combine PCA and Clustering

Our PCA results were in `wisc.pr$x`

```{r}
#distance matrix from PCA result
d <- dist( wisc.pr$x[,1:3])
hc <- hclust (d, method ="ward.D2")
plot(hc)
```

Cut tree into two groups/branches/clusters...

```{r}
grps <- cutree(hc, k=2)
```

```{r}
plot(wisc.pr$x, col=grps)
```

Compare my clustering result (my `grps`) to the expert `diagnosis`

```{r}
table(diagnosis)
```

```{r}
table(grps)
```

```{r}
table(diagnosis, grps)
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Single

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

It does a better job in separating B to M by proportion of the cluster






